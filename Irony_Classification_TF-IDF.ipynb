{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thavyne-KDR/Detec-o_de_Ironia_em_Textos/blob/main/Irony_Classification_TF-IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cc3ad60"
      },
      "source": [
        "!pip install -q scikit-learn pandas numpy\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import zipfile, os\n",
        "\n",
        "zip_path = next(iter(uploaded.keys()))\n",
        "extract_path = \"dados_ironia\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_path)\n",
        "\n",
        "os.listdir(extract_path)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "train = pd.read_json(f\"{extract_path}/train.jsonl\", lines=True)\n",
        "test  = pd.read_csv(f\"{extract_path}/test.csv\")\n",
        "sample = pd.read_csv(f\"{extract_path}/sample_submission.csv\")\n",
        "\n",
        "print(\"Train:\", train.shape, train.columns.tolist())\n",
        "print(\"Test :\", test.shape , test.columns.tolist())\n",
        "print(\"Sample submission:\", sample.columns.tolist())\n",
        "\n",
        "display(train.head(3))\n",
        "display(test.head(3))\n",
        "display(sample.head(3))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "    train['text'], train['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train_text.shape, y_train.shape)\n",
        "print(\"Validation:\", X_val_text.shape, y_val.shape)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train = tfidf.fit_transform(X_train_text)\n",
        "X_val = tfidf.transform(X_val_text)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "modelos = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
        "    \"LinearSVC\": LinearSVC(class_weight=\"balanced\"),\n",
        "    \"MultinomialNB\": MultinomialNB()\n",
        "}\n",
        "\n",
        "resultados = []\n",
        "for nome, mdl in modelos.items():\n",
        "    mdl.fit(X_train, y_train)\n",
        "    pred = mdl.predict(X_val)\n",
        "    bal_acc = balanced_accuracy_score(y_val, pred)\n",
        "    resultados.append((nome, bal_acc))\n",
        "    print(f\"{nome}: Balanced Accuracy = {bal_acc:.4f}\")\n",
        "\n",
        "resultados.sort(key=lambda x: x[1], reverse=True)\n",
        "melhor_nome, melhor_score = resultados[0]\n",
        "print(\"\\nMelhor modelo:\", melhor_nome, \"| Balanced Accuracy:\", round(melhor_score, 4))\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "\n",
        "modelos[\"RandomForestClassifier\"] = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
        "modelos[\"GradientBoostingClassifier\"] = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "modelos[\"AdaBoostClassifier\"] = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "\n",
        "resultados = []\n",
        "for nome, mdl in modelos.items():\n",
        "    mdl.fit(X_train, y_train)\n",
        "    pred = mdl.predict(X_val)\n",
        "    bal_acc = balanced_accuracy_score(y_val, pred)\n",
        "    resultados.append((nome, bal_acc))\n",
        "    print(f\"{nome}: Balanced Accuracy = {bal_acc:.4f}\")\n",
        "\n",
        "resultados.sort(key=lambda x: x[1], reverse=True)\n",
        "melhor_nome, melhor_score = resultados[0]\n",
        "print(\"\\nMelhor modelo:\", melhor_nome, \"| Balanced Accuracy:\", round(melhor_score, 4))\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_svc = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'loss': ['hinge', 'squared_hinge'],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "grid_search_svc = GridSearchCV(LinearSVC(class_weight=\"balanced\"), param_grid_svc, cv=3, scoring='balanced_accuracy', n_jobs=-1)\n",
        "grid_search_svc.fit(X_train, y_train)\n",
        "\n",
        "best_svc_model = grid_search_svc.best_estimator_\n",
        "svc_tuned_pred = best_svc_model.predict(X_val)\n",
        "svc_tuned_bal_acc = balanced_accuracy_score(y_val, svc_tuned_pred)\n",
        "\n",
        "print(f\"Tuned LinearSVC: Best Parameters = {grid_search_svc.best_params_}\")\n",
        "print(f\"Tuned LinearSVC: Balanced Accuracy = {svc_tuned_bal_acc:.4f}\")\n",
        "\n",
        "grid_search_lr = GridSearchCV(LogisticRegression(max_iter=1000, class_weight=\"balanced\"), param_grid_lr, cv=3, scoring='balanced_accuracy', n_jobs=-1)\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "\n",
        "best_lr_model = grid_search_lr.best_estimator_\n",
        "lr_tuned_pred = best_lr_model.predict(X_val)\n",
        "lr_tuned_bal_acc = balanced_accuracy_score(y_val, lr_tuned_pred)\n",
        "\n",
        "print(f\"Tuned LogisticRegression: Best Parameters = {grid_search_lr.best_params_}\")\n",
        "print(f\"Tuned LogisticRegression: Balanced Accuracy = {lr_tuned_bal_acc:.4f}\")\n",
        "\n",
        "!pip install -q transformers sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_name = 'paraphrase-MiniLM-L3-v2'\n",
        "sbert_model = SentenceTransformer(model_name)\n",
        "\n",
        "print(\"Gerando embeddings para os dados de treinamento...\")\n",
        "X_train_embeddings = sbert_model.encode(X_train_text.tolist(), show_progress_bar=True)\n",
        "\n",
        "print(\"Gerando embeddings para os dados de validação...\")\n",
        "X_val_embeddings = sbert_model.encode(X_val_text.tolist(), show_progress_bar=True)\n",
        "\n",
        "print(\"\\nDimensões dos embeddings de treinamento:\", X_train_embeddings.shape)\n",
        "print(\"Dimensões dos embeddings de validação:\", X_val_embeddings.shape)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "modelos_embeddings = {\n",
        "    \"LogisticRegression_Embeddings\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
        "    \"LinearSVC_Embeddings\": LinearSVC(class_weight=\"balanced\"),\n",
        "    \"RandomForestClassifier_Embeddings\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\"),\n",
        "    \"GradientBoostingClassifier_Embeddings\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    \"AdaBoostClassifier_Embeddings\": AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "}\n",
        "\n",
        "resultados_embeddings = []\n",
        "for nome, mdl in modelos_embeddings.items():\n",
        "    print(f\"Treinando {nome}...\")\n",
        "    mdl.fit(X_train_embeddings, y_train)\n",
        "    pred = mdl.predict(X_val_embeddings)\n",
        "    bal_acc = balanced_accuracy_score(y_val, pred)\n",
        "    resultados_embeddings.append((nome, bal_acc))\n",
        "    print(f\"{nome}: Balanced Accuracy = {bal_acc:.4f}\\n\")\n",
        "\n",
        "resultados_embeddings.sort(key=lambda x: x[1], reverse=True)\n",
        "melhor_nome_embeddings, melhor_score_embeddings = resultados_embeddings[0]\n",
        "print(\"\\nMelhor modelo com Embeddings:\", melhor_nome_embeddings, \"| Balanced Accuracy:\", round(melhor_score_embeddings, 4))\n",
        "\n",
        "import string\n",
        "\n",
        "def count_punctuation(text):\n",
        "    return sum([1 for char in text if char in string.punctuation])\n",
        "\n",
        "train['text_length'] = train['text'].apply(len)\n",
        "test['text_length'] = test['text'].apply(len)\n",
        "\n",
        "train['punctuation_count'] = train['text'].apply(count_punctuation)\n",
        "test['punctuation_count'] = test['text'].apply(count_punctuation)\n",
        "\n",
        "display(train.head())\n",
        "display(test.head())\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X_train_numerical = train.loc[X_train_text.index, ['text_length', 'punctuation_count']]\n",
        "X_val_numerical = train.loc[X_val_text.index, ['text_length', 'punctuation_count']]\n",
        "X_test_numerical = test[['text_length', 'punctuation_count']]\n",
        "\n",
        "X_train_combined = hstack([X_train, X_train_numerical.values])\n",
        "X_val_combined = hstack([X_val, X_val_numerical.values])\n",
        "X_test_combined = hstack([X_test, X_test_numerical.values])\n",
        "\n",
        "print(\"Dimensões do conjunto de treinamento combinado:\", X_train_combined.shape)\n",
        "print(\"Dimensões do conjunto de validação combinado:\", X_val_combined.shape)\n",
        "print(\"Dimensões do conjunto de teste combinado:\", X_test_combined.shape)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "modelos_combinados = {\n",
        "    \"LogisticRegression_Combined\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
        "    \"LinearSVC_Combined\": LinearSVC(class_weight=\"balanced\")\n",
        "}\n",
        "\n",
        "resultados_combinados = []\n",
        "for nome, mdl in modelos_combinados.items():\n",
        "    print(f\"Treinando {nome} com características combinadas...\")\n",
        "    mdl.fit(X_train_combined, y_train)\n",
        "    pred = mdl.predict(X_val_combined)\n",
        "    bal_acc = balanced_accuracy_score(y_val, pred)\n",
        "    resultados_combinados.append((nome, bal_acc))\n",
        "    print(f\"{nome}: Balanced Accuracy = {bal_acc:.4f}\\n\")\n",
        "\n",
        "resultados_combinados.sort(key=lambda x: x[1], reverse=True)\n",
        "melhor_nome_combinado, melhor_score_combinado = resultados_combinados[0]\n",
        "print(\"\\nMelhor modelo com características combinadas:\", melhor_nome_combinado, \"| Balanced Accuracy:\", round(melhor_score_combinado, 4))\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "best_model_for_cv = best_lr_model\n",
        "\n",
        "X_full_tfidf = tfidf.fit_transform(train[\"text\"])\n",
        "y_full = train[\"label\"]\n",
        "\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"Realizando validação cruzada ({n_splits} folds) para {type(best_model_for_cv).__name__}...\")\n",
        "cv_scores = cross_val_score(best_model_for_cv, X_full_tfidf, y_full, cv=skf, scoring='balanced_accuracy', n_jobs=-1)\n",
        "\n",
        "print(f\"\\nAcurácia Balanceada para cada fold: {cv_scores}\")\n",
        "print(f\"Média da Acurácia Balanceada na validação cruzada: {cv_scores.mean():.4f}\")\n",
        "print(f\"Desvio padrão da Acurácia Balanceada na validação cruzada: {cv_scores.std():.4f}\")\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', best_lr_model), ('svc', best_svc_model)],\n",
        "    voting='hard',\n",
        "    weights=[1, 1],\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Treinando o Voting Classifier...\")\n",
        "voting_clf.fit(X_full_tfidf, y_full)\n",
        "\n",
        "print(\"Avaliando o Voting Classifier no conjunto de validação...\")\n",
        "voting_pred = voting_clf.predict(X_val)\n",
        "voting_bal_acc = balanced_accuracy_score(y_val, voting_pred)\n",
        "\n",
        "print(f\"\\nVoting Classifier (LR + SVC): Balanced Accuracy = {voting_bal_acc:.4f}\")\n",
        "\n",
        "!pip install -q tensorflow\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "num_words = 10000\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
        "\n",
        "print(\"Ajustando o tokenizador aos dados de treinamento...\")\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "print(\"Convertendo textos em sequências...\")\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train_text)\n",
        "val_sequences = tokenizer.texts_to_sequences(X_val_text)\n",
        "\n",
        "train_text_lengths = [len(text.split()) for text in X_train_text]\n",
        "max_len = int(np.mean(train_text_lengths) + 2 * np.std(train_text_lengths))\n",
        "print(f\"Comprimento máximo definido para as sequências: {max_len}\")\n",
        "\n",
        "print(\"Preenchendo as sequências...\")\n",
        "X_train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "X_val_padded = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nDimensões das sequências de treinamento preenchidas:\", X_train_padded.shape)\n",
        "print(\"Dimensões das sequências de validação preenchidas:\", X_val_padded.shape)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Tamanho do vocabulário: {vocab_size}\")\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "print(f\"Dimensão dos embeddings: {embedding_dim}\")\n",
        "print(f\"Comprimento máximo da sequência: {max_len}\")\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def balanced_accuracy(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "\n",
        "    true_positives = tf.reduce_sum(y_true * y_pred)\n",
        "    true_negatives = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
        "    false_positives = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    false_negatives = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "\n",
        "    sensitivity = true_positives / (true_positives + false_negatives + tf.keras.backend.epsilon())\n",
        "    specificity = true_negatives / (true_negatives + false_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "    return (sensitivity + specificity) / 2\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[balanced_accuracy])\n",
        "\n",
        "print(\"Modelo compilado com sucesso.\")\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_balanced_accuracy', patience=5, mode='max', restore_best_weights=True)\n",
        "\n",
        "print(f\"Iniciando o treinamento por até {epochs} épocas com batch size {batch_size} e Early Stopping...\")\n",
        "\n",
        "history = model.fit(X_train_padded, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_val_padded, y_val),\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=1)\n",
        "\n",
        "print(\"\\nTreinamento concluído.\")\n",
        "\n",
        "print(\"Avaliando o modelo de Deep Learning no conjunto de validação...\")\n",
        "loss, bal_acc_dl = model.evaluate(X_val_padded, y_val, verbose=0)\n",
        "\n",
        "print(f\"\\nDeep Learning Model: Balanced Accuracy = {bal_acc_dl:.4f}\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Preparando os dados combinando headline e text...\")\n",
        "\n",
        "train['input_text'] = train['headline'] + ' [SEP] ' + train['text']\n",
        "test['input_text'] = test['text']\n",
        "\n",
        "train['input_text'] = train['input_text'].fillna('')\n",
        "test['input_text'] = test['input_text'].fillna('')\n",
        "\n",
        "y_full = train['label']\n",
        "\n",
        "print(\"Dados preparados com sucesso!\")\n",
        "print(\"\\nExemplo do input de treino combinado:\")\n",
        "print(train['input_text'].iloc[0])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"Treinando o Modelo 1: Regressão Logística...\")\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_full_tfidf = tfidf.fit_transform(train['input_text'])\n",
        "X_test_tfidf = tfidf.transform(test['input_text'])\n",
        "\n",
        "best_lr_model = LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=100, penalty='l2', solver='liblinear')\n",
        "\n",
        "best_lr_model.fit(X_full_tfidf, y_full)\n",
        "\n",
        "preds_lr_proba = best_lr_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "print(\"Modelo 1 treinado e previsões de probabilidade geradas!\")\n",
        "print(\"Shape das previsões:\", preds_lr_proba.shape)\n",
        "\n",
        "preds_lr_class = (preds_lr_proba > 0.5).astype(int)\n",
        "submission_lr = pd.DataFrame({'id': test['id'], 'label': preds_lr_class})\n",
        "submission_lr.to_csv('submission_logistic_regression_only.csv', index=False)\n",
        "files.download('submission_logistic_regression_only.csv')\n",
        "\n",
        "\n",
        "extract_path = \"dados_ironia\"\n",
        "train = pd.read_json(f\"{extract_path}/train.jsonl\", lines=True)\n",
        "test = pd.read_csv(f\"{extract_path}/test.csv\")\n",
        "\n",
        "best_lr_model_full_train = LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=100, penalty='l2', solver='liblinear')\n",
        "\n",
        "tfidf_full = TfidfVectorizer(max_features=5000)\n",
        "X_full_tfidf = tfidf_full.fit_transform(train['text'])\n",
        "\n",
        "y_full = train['label']\n",
        "print(\"Treinando o melhor modelo (Logistic Regression com TF-IDF sintonizado) em todo o conjunto de dados de treinamento...\")\n",
        "best_lr_model_full_train.fit(X_full_tfidf, y_full)\n",
        "\n",
        "print(\"Treinamento concluído.\")\n",
        "\n",
        "X_test_tfidf = tfidf_full.transform(test['text'].fillna(''))\n",
        "\n",
        "preds = best_lr_model_full_train.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Previsões geradas para o conjunto de teste.\")\n",
        "\n",
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test['id']\n",
        "\n",
        "submission['label'] = preds.astype(int)\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "display(submission.head())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}